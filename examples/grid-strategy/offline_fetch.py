import datetime
import os

import arcticdb as adb

#!/usr/bin/env python
"""æ‰¹é‡ä¸‹è½½å¤šä¸ªå¸ç§çš„ 5åˆ†é’Ÿ Kçº¿æ•°æ®ï¼ˆæ¥è‡ª Binance ZIP æ–‡ä»¶ï¼‰ï¼Œå¹¶åˆå¹¶ä¿å­˜ä¸º CSV"""

import zipfile
from datetime import datetime, timedelta

import pandas as pd
import requests
from tqdm import tqdm


def ensure_directory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)


def download_zip_monthly(symbol, interval, year, month, output_dir):
    filename = f"{symbol}-{interval}-{year}-{month:02d}.zip"
    url = f"https://data.binance.vision/data/spot/monthly/klines/{symbol}/{interval}/{filename}"
    local_path = os.path.join(output_dir, filename)

    if os.path.exists(local_path):
        print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
        return True

    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        total = int(response.headers.get("content-length", 0))
        with (
            open(local_path, "wb") as f,
            tqdm(desc=filename, total=total, unit="B", unit_scale=True) as bar,
        ):
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                bar.update(len(chunk))
        print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
        return True
    except requests.exceptions.RequestException as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {filename} - {e}")
        return False


def extract_and_read_csv(zip_path, output_dir):
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(output_dir)
        all_dfs = []
        for name in zf.namelist():
            csv_path = os.path.join(output_dir, name)
            df = pd.read_csv(csv_path, header=None)
            all_dfs.append(df)
            os.remove(csv_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        return all_dfs


def download_and_merge(symbol, interval, start_date, end_date, output_dir):
    all_data = []
    zip_files = []  # Track zip files for cleanup
    current_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")

    while current_dt <= end_dt:
        year, month = current_dt.year, current_dt.month
        success = download_zip_monthly(symbol, interval, year, month, output_dir)
        if success:
            zip_file = os.path.join(
                output_dir, f"{symbol}-{interval}-{year}-{month:02d}.zip"
            )
            zip_files.append(zip_file)  # Add to cleanup list
            dfs = extract_and_read_csv(zip_file, output_dir)
            all_data.extend(dfs)
        current_dt += timedelta(days=32)
        current_dt = current_dt.replace(day=1)

    # Cleanup zip files
    for zip_file in zip_files:
        try:
            os.remove(zip_file)
            print(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {zip_file}")
        except OSError as e:
            print(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {zip_file}: {e}")

    if all_data:
        df = pd.concat(all_data, ignore_index=True)
        df.columns = [
            "Open Time",
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "Close Time",
            "Quote Asset Volume",
            "Number of Trades",
            "Taker Buy Base Volume",
            "Taker Buy Quote Volume",
            "Ignore",
        ]
        return df
    print(f"âš ï¸ {symbol} æ²¡æœ‰æ•°æ®å¯ç”¨")
    return None


def check_data_coverage(library, symbol_key, start_date, end_date, interval):
    """é€æ¡æ£€æŸ¥æ•°æ®åº“ä¸­æ•°æ®æ˜¯å¦å®Œå…¨è¦†ç›–æŒ‡å®šçš„æ—¶é—´èŒƒå›´ï¼Œç¡®ä¿æ²¡æœ‰æ•°æ®ç¼ºå£

    Args:
        library: Arcticåº“å¯¹è±¡
        symbol_key: æ•°æ®çš„é”®å
        start_date: ç›®æ ‡å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç›®æ ‡ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        interval: æ—¶é—´é—´éš” (ä¾‹å¦‚ "1m", "5m", "1h")

    Returns:
        tuple: (is_covered, missing_periods, existing_df)
            - is_covered: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦å®Œå…¨è¦†ç›–
            - missing_periods: ç¼ºå¤±æ•°æ®çš„æ—¶é—´æ®µåˆ—è¡¨ [(start1, end1), (start2, end2), ...]
            - existing_df: æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºDataFrame
    """
    from datetime import datetime, timedelta

    import pandas as pd

    # è§£ææ—¶é—´é—´éš”
    interval_map = {
        "1s": timedelta(seconds=1),
        "1m": timedelta(minutes=1),
        "5m": timedelta(minutes=5),
        "15m": timedelta(minutes=15),
        "1h": timedelta(hours=1),
        "4h": timedelta(hours=4),
        "1d": timedelta(days=1),
    }

    if interval not in interval_map:
        print(f"âš ï¸ ä¸æ”¯æŒçš„æ—¶é—´é—´éš”: {interval}ï¼Œä½¿ç”¨é»˜è®¤é—´éš”5åˆ†é’Ÿ")
        interval_delta = timedelta(minutes=5)
    else:
        interval_delta = interval_map[interval]

    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")

    # è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
    start_timestamp = int(start_dt.timestamp() * 1000000)
    end_timestamp = int(end_dt.timestamp() * 1000000)

    print(start_timestamp, end_timestamp)

    if symbol_key in library.list_symbols():
        try:
            # è¯»å–æ•°æ®
            arctic_result = library.read(symbol_key)
            existing_df = arctic_result.data

            if existing_df.empty:
                print(f"æ•°æ®åº“ä¸­ {symbol_key} çš„æ•°æ®ä¸ºç©º")
                return False, [(start_dt, end_dt)], existing_df

            # æ£€æŸ¥ç´¢å¼•ç±»å‹å¹¶ç¡®ä¿æ˜¯æ—¶é—´æˆ³æ ¼å¼
            print(f"å½“å‰ç´¢å¼•: {(existing_df.index)}")

            # å¤„ç†ç´¢å¼•é—®é¢˜
            if "Close Time" in existing_df.columns:
                print("'Close Time'åœ¨åˆ—ä¸­ï¼Œè®¾ç½®ä¸ºç´¢å¼•")
                existing_df = existing_df.reset_index()
                existing_df.set_index("Close Time", inplace=True)

            # ç¡®ä¿ç´¢å¼•æ˜¯æŒ‰æ—¶é—´æ’åºçš„
            existing_df = existing_df.sort_index()

            # è·å–æ•°æ®åº“ä¸­çš„æœ€æ—©å’Œæœ€æ™šæ—¶é—´æˆ³
            db_min_time = existing_df.index.min()
            db_max_time = existing_df.index.max()
            print(db_min_time, db_max_time)

            print(
                f"æ•°æ®åº“ä¸­çš„æ•´ä½“æ—¶é—´èŒƒå›´: {datetime.fromtimestamp(int(db_min_time / 1000000))} åˆ° {datetime.fromtimestamp(int(db_max_time / 1000000))}"
            )
            print(f"è¯·æ±‚çš„æ—¶é—´èŒƒå›´: {start_dt} åˆ° {end_dt}")

            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¦†ç›–äº†è¯·æ±‚çš„æ—¶é—´èŒƒå›´
            is_in_range = (db_min_time <= start_timestamp) and (
                db_max_time >= end_timestamp
            )

            if not is_in_range:
                print("âš ï¸ æ•°æ®åº“æ—¶é—´èŒƒå›´ä¸å®Œå…¨è¦†ç›–è¯·æ±‚çš„æ—¶é—´èŒƒå›´")
                missing_periods = []

                # æ·»åŠ å‰æœŸç¼ºå¤±åŒºé—´
                if db_min_time > start_timestamp:
                    missing_periods.append(
                        (start_dt, datetime.fromtimestamp(db_min_time / 1000))
                    )
                    print(
                        f"  ç¼ºå°‘å‰æœŸæ•°æ®: {start_dt} åˆ° {datetime.fromtimestamp(db_min_time / 1000)}"
                    )

                # æ·»åŠ åæœŸç¼ºå¤±åŒºé—´
                if db_max_time < end_timestamp:
                    missing_periods.append(
                        (datetime.fromtimestamp(db_max_time / 1000), end_dt)
                    )
                    print(
                        f"  ç¼ºå°‘åæœŸæ•°æ®: {datetime.fromtimestamp(db_max_time / 1000)} åˆ° {end_dt}"
                    )

                return False, missing_periods, existing_df

            # é€æ¡æ£€æŸ¥æ•°æ®è¿ç»­æ€§
            print("æ­£åœ¨é€æ¡æ£€æŸ¥æ•°æ®è¿ç»­æ€§...")

            # å°†ç´¢å¼•è½¬æ¢ä¸ºdatetimeï¼Œä¾¿äºå¤„ç†
            existing_df_datetime = existing_df.reset_index()
            existing_df_datetime["datetime"] = pd.to_datetime(
                existing_df_datetime["Close Time"], unit="us"
            )
            existing_df_datetime = existing_df_datetime.sort_values("datetime")

            # é™åˆ¶åœ¨è¯·æ±‚çš„æ—¶é—´èŒƒå›´å†…
            mask = (existing_df_datetime["datetime"] >= start_dt) & (
                existing_df_datetime["datetime"] <= end_dt
            )
            filtered_df = existing_df_datetime[mask]

            if filtered_df.empty:
                print("âš ï¸ è¿‡æ»¤åçš„æ•°æ®ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ—¥æœŸèŒƒå›´é—®é¢˜")
                return False, [(start_dt, end_dt)], existing_df

            # æ£€æŸ¥æ¯ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çš„é—´éš”
            missing_periods = []
            current_dt = filtered_df["datetime"].iloc[0]
            expected_dt = start_dt

            # å…ˆæ£€æŸ¥å¼€å§‹æ—¶é—´
            if current_dt > expected_dt + interval_delta:
                missing_periods.append((expected_dt, current_dt - interval_delta))
                print(f"  ç¼ºå°‘æ•°æ®: {expected_dt} åˆ° {current_dt - interval_delta}")

            # æ£€æŸ¥æ‰€æœ‰æ•°æ®ç‚¹é—´éš”
            for i in range(1, len(filtered_df)):
                prev_dt = filtered_df["datetime"].iloc[i - 1]
                curr_dt = filtered_df["datetime"].iloc[i]

                # é¢„æœŸçš„ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
                expected_next_dt = prev_dt + interval_delta

                # å¦‚æœå®é™…æ—¶é—´ç‚¹æ™šäºé¢„æœŸï¼Œè¯´æ˜æœ‰ç¼ºå£
                if curr_dt > expected_next_dt + timedelta(seconds=1):  # å…è®¸1ç§’è¯¯å·®
                    missing_periods.append((expected_next_dt, curr_dt - interval_delta))
                    print(
                        f"  å‘ç°æ•°æ®ç¼ºå£: {expected_next_dt} åˆ° {curr_dt - interval_delta}"
                    )

            # æ£€æŸ¥ç»“æŸæ—¶é—´
            last_dt = filtered_df["datetime"].iloc[-1]
            if last_dt < end_dt - interval_delta:
                missing_periods.append((last_dt + interval_delta, end_dt))
                print(f"  ç¼ºå°‘æ•°æ®: {last_dt + interval_delta} åˆ° {end_dt}")

            if missing_periods:
                print(f"âš ï¸ å…±å‘ç° {len(missing_periods)} ä¸ªæ•°æ®ç¼ºå£")
                return False, missing_periods, existing_df
            print("âœ“ æ•°æ®è¿ç»­æ€§æ£€æŸ¥é€šè¿‡ï¼Œå®Œå…¨è¦†ç›–è¯·æ±‚çš„æ—¶é—´èŒƒå›´")
            return True, [], existing_df

        except Exception as e:
            print(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {e}")
            return False, [(start_dt, end_dt)], pd.DataFrame()
    else:
        print(f"æ•°æ®åº“ä¸­æ²¡æœ‰ {symbol_key} çš„æ•°æ®")
        return False, [(start_dt, end_dt)], pd.DataFrame()


def get_all_timestamps_in_range(library, coins, interval, start_date, end_date):
    """è·å–ç‰¹å®šæ—¥æœŸèŒƒå›´å†…æ‰€æœ‰å¸ç§çš„Kçº¿æ—¶é—´æˆ³"""
    from datetime import datetime

    # è½¬æ¢æ—¥æœŸä¸ºæ—¶é—´æˆ³ (å¾®ç§’çº§)
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    start_timestamp = int(start_dt.timestamp() * 1000000)
    end_timestamp = int(end_dt.timestamp() * 1000000)

    all_timestamps = []
    for coin in coins:
        symbol_key = f"{coin}_klines_{interval}"
        if symbol_key in library.list_symbols():
            print(f"è¯»å– {symbol_key} çš„æ—¶é—´æˆ³...")
            arctic_result = library.read(symbol_key)
            df = arctic_result.data

            # ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ—¶é—´æˆ³
            filtered_timestamps = [
                ts for ts in df.index if start_timestamp <= ts <= end_timestamp
            ]
            print(f"  {symbol_key} åœ¨æ—¥æœŸèŒƒå›´å†…æœ‰ {len(filtered_timestamps)} ä¸ªæ—¶é—´æˆ³")
            all_timestamps.extend(filtered_timestamps)

    # å»é‡å¹¶æ’åº
    unique_timestamps = sorted(list(set(all_timestamps)))
    print(f"æ€»å…±è·å–åˆ° {len(unique_timestamps)} ä¸ªå”¯ä¸€æ—¶é—´æˆ³")

    # å°†å¾®ç§’æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ—¥æœŸæ—¶é—´è¿›è¡Œå±•ç¤º
    if unique_timestamps:
        first_time = datetime.fromtimestamp(unique_timestamps[0] / 1000000)
        last_time = datetime.fromtimestamp(unique_timestamps[-1] / 1000000)
        print(f"æ—¶é—´æˆ³èŒƒå›´: {first_time} åˆ° {last_time}")

        # æ˜¾ç¤ºéƒ¨åˆ†æ ·æœ¬
        print("æ—¶é—´æˆ³æ ·æœ¬:")
        for i in range(min(5, len(unique_timestamps))):
            dt = datetime.fromtimestamp(unique_timestamps[i] / 1000000)
            print(f"  {i + 1}. {dt} ({unique_timestamps[i]})")

    return unique_timestamps


def fetch_for_offline(coins, interval, start_date, end_date, output_dir, library):
    ensure_directory(output_dir)
    for coin in coins:
        symbol = coin
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {symbol} â€¦")

        # ä»æ•°æ®åº“è¯»å–ç°æœ‰æ•°æ®
        symbol_key = f"{coin}_klines_{interval}"

        # æ£€æŸ¥æ•°æ®è¦†ç›–æƒ…å†µï¼Œé€æ¡æ£€æŸ¥è¿ç»­æ€§
        is_covered, missing_periods, from_storage_df = check_data_coverage(
            library, symbol_key, start_date, end_date, interval
        )
        storage_length = len(from_storage_df)

        if is_covered:
            print("âœ… æ•°æ®åº“å·²æœ‰å®Œæ•´è¿ç»­æ•°æ®ï¼Œè·³è¿‡ä¸‹è½½")
            # å¯ä»¥é€‰æ‹©å±•ç¤ºä¸€äº›æ•°æ®é¢„è§ˆ
            print("\nç°æœ‰æ•°æ®é¢„è§ˆ:")
            print(from_storage_df.head(5))
            continue

        # æ•°æ®ä¸å®Œæ•´æˆ–ä¸è¿ç»­ï¼Œéœ€è¦ä¸‹è½½
        print("ğŸ“¥ éœ€è¦ä¸‹è½½æ–°æ•°æ®æ¥å¡«è¡¥æ•°æ®ç¼ºå£")

        # ä¸‹è½½æ•°æ®å¹¶ç»§ç»­åŸæ¥çš„å¤„ç†é€»è¾‘...
        # å¦‚æœè¦é’ˆå¯¹å…·ä½“çš„ç¼ºå¤±åŒºé—´ä¸‹è½½ï¼Œå¯ä»¥éå† missing_periods
        for start_period, end_period in missing_periods:
            period_start = start_period.strftime("%Y-%m-%d")
            period_end = end_period.strftime("%Y-%m-%d")
            print(f"  ä¸‹è½½æ•°æ®: {period_start} åˆ° {period_end}")

        if symbol_key in library.list_symbols():
            try:
                # è¯»å–æ•°æ®
                arctic_result = library.read(symbol_key)
                from_storage_df = arctic_result.data

                # æ£€æŸ¥å½“å‰ç´¢å¼•æ˜¯ä»€ä¹ˆ
                print(f"å½“å‰ç´¢å¼•ç±»å‹: {type(from_storage_df.index)}")
                print(f"å½“å‰åˆ—: {from_storage_df.columns.tolist()}")

                # æ£€æŸ¥'Close Time'æ˜¯å¦å·²ç»æ˜¯ç´¢å¼•
                if "Close Time" in from_storage_df.columns:
                    print("'Close Time'åœ¨åˆ—ä¸­ï¼Œè®¾ç½®ä¸ºç´¢å¼•")
                    from_storage_df = from_storage_df.reset_index()
                    from_storage_df.set_index("Close Time", inplace=True)
                else:
                    print("'Close Time'ä¸åœ¨åˆ—ä¸­ï¼Œå¯èƒ½å·²ç»æ˜¯ç´¢å¼•æˆ–ä¸å­˜åœ¨")

                storage_length = len(from_storage_df)
                print(f"ç°æœ‰æ•°æ®æ¡æ•°: {storage_length} è¡Œ")
            except Exception as e:
                print(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {e}")
                from_storage_df = pd.DataFrame()
                storage_length = 0
        else:
            from_storage_df = pd.DataFrame()
            storage_length = 0
            print(f"æ•°æ®åº“ä¸­æ²¡æœ‰ {symbol_key} çš„æ•°æ®")

        # ä¸‹è½½æ–°æ•°æ®
        new_df = download_and_merge(symbol, interval, start_date, end_date, output_dir)
        if new_df is not None:
            # è®¾ç½®ç´¢å¼•ä»¥ä¾¿åˆå¹¶
            new_df = new_df.reset_index()

            new_df.set_index("Close Time", inplace=True)
            new_length = len(new_df)
            print(f"æ–°ä¸‹è½½æ•°æ®æ¡æ•°: {new_length} è¡Œ")

            # åˆå¹¶æ•°æ®æ¡†ï¼Œä½¿ç”¨ç´¢å¼•å»é‡
            if not from_storage_df.empty:
                # åˆå¹¶å¹¶ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œç„¶ååˆ é™¤é‡å¤ç´¢å¼•ï¼Œä¿ç•™æœ€åä¸€ä¸ªå‡ºç°çš„è¡Œ
                combined_df = pd.concat([from_storage_df, new_df])
                combined_df = combined_df[~combined_df.index.duplicated(keep="last")]
                combined_length = len(combined_df)

                print(f"åˆå¹¶å‰æ€»æ¡æ•°: {storage_length + new_length} è¡Œ")
                print(f"åˆå¹¶åæ€»æ¡æ•°: {combined_length} è¡Œ")
                print(f"é‡å¤æ¡æ•°: {storage_length + new_length - combined_length} è¡Œ")
            else:
                combined_df = new_df
                combined_length = new_length
                print(f"é¦–æ¬¡åˆ›å»ºæ•°æ®ï¼Œæ€»æ¡æ•°: {combined_length} è¡Œ")

            # ä¿å­˜åˆ°CSVå’Œæ•°æ®åº“
            output_csv = os.path.join(output_dir, f"{symbol_key}.csv")
            combined_df.to_csv(output_csv)
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_csv}")

            # å†™å…¥æ•°æ®åº“
            library.write(f"{symbol_key}", combined_df)
            print(f"âœ… æ•°æ®å·²å†™å…¥æ•°æ®åº“ {symbol_key}")

            # æ˜¾ç¤ºåˆå¹¶åçš„å‰å‡ è¡Œæ•°æ®
            print("\nåˆå¹¶åæ•°æ®é¢„è§ˆ:")
            print(combined_df.head(5))
        else:
            print(f"âš ï¸ æ²¡æœ‰æ–°æ•°æ®ä¸‹è½½ï¼Œä¿æŒç°æœ‰æ•°æ®ä¸å˜")
    
    # timestamps = get_all_timestamps_in_range(library, coins, interval, start_date, end_date)
    # return timestamps


if __name__ == "__main__":
    # this will set up the storage using the local file system
    uri = "lmdb://examples/grid-strategy/data/database_offline/"
    ac = adb.Arctic(uri)

    library = ac.get_library("market", create_if_missing=True)
    print(ac.list_libraries())

    coins = ["BTCUSDT", "DOGEUSDT"]
    interval = "5m"
    start_date = "2025-01-01"
    end_date = "2025-06-01"
    output_dir = "/Users/lizeyu/Desktop/qoc/tmp/raw/5m_klines_raw"

    fetch_for_offline(coins, interval, start_date, end_date, output_dir, library)
